{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "33d0fd4e",
      "metadata": {
        "id": "33d0fd4e"
      },
      "source": [
        "\n",
        "# Standalone Transformers SFT Notebook (Chat-format, Assistant-only Loss)\n",
        "\n",
        "This notebook trains a causal language model using **Hugging Face Transformers** (no TRL).  \n",
        "It expects a dataset with a `messages` column: a list of chat turns like\n",
        "```json\n",
        "{\"messages\": [\n",
        "  {\"role\": \"system\", \"content\": \"You are helpful.\"},\n",
        "  {\"role\": \"user\", \"content\": \"Say hi.\"},\n",
        "  {\"role\": \"assistant\", \"content\": \"Hello!\"}\n",
        "]}\n",
        "```\n",
        "**Features**\n",
        "- Uses the model's `chat_template` (`tokenizer.apply_chat_template`) to format messages.\n",
        "- Masks the loss to **only** the last assistant turn (assistant-only loss) without TRL.\n",
        "- Optional **LoRA** (PEFT) and optional **4-bit** quantization (bitsandbytes).\n",
        "- Works with Qwen/TinyLlama/phi-3-mini etc. (set `MODEL_ID`).\n",
        "- Includes a **toy dataset** if you don't have one yet.\n",
        "\n",
        "> Tip: If your `content` pieces are structured (e.g., list of parts), the normalizer flattens text parts.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ],
      "metadata": {
        "id": "t_PR6O5pQznf"
      },
      "id": "t_PR6O5pQznf",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4QY-sTSVamMz",
      "metadata": {
        "collapsed": true,
        "id": "4QY-sTSVamMz"
      },
      "outputs": [],
      "source": [
        "%pip install nbstripout\n",
        "%pip install -U \"huggingface-hub>=0.34.0,<1.0\"\n",
        "%pip check\n",
        "%pip install hf_transfer\n",
        "%pip install -U  datasets accelerate peft trl bitsandbytes peft flash-attn\n",
        "%pip install -U \"trl>=0.10.0\" \"transformers>=4.44.0\"\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "!export HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "import gc\n",
        "from google.colab import runtime\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import math\n",
        "import json, random\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "from transformers import AutoTokenizer, pipeline, EarlyStoppingCallback\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import os\n",
        "import wandb\n",
        "import shutil\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login, whoami\n",
        "os.environ[\"WANDB_DISABLED\"] = \"false\"  # or \"true\" to mute\n",
        "os.environ[\"WANDB_PROJECT\"]   = \"qwen3coder-finetune-fp16\"\n",
        "\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # mitigate fragmentation\n",
        "REPO_URL=\"https://github.com/UH-Insure/Colab-Training.git\"\n",
        "REPO=\"Colab-Training\"\n",
        "\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "# If repo exists, update it; otherwise, clone fresh\n",
        "if os.path.exists(REPO):\n",
        "    print(f\"Repo '{REPO}' exists, pulling latest changes...\")\n",
        "    os.chdir(REPO)\n",
        "    !git reset --hard HEAD   # optional: discard local changes\n",
        "    !git pull\n",
        "else:\n",
        "    print(f\"Cloning repo '{REPO}'...\")\n",
        "    !git clone \"$REPO_URL\" \"$REPO\"\n",
        "    os.chdir(REPO)\n",
        "\n",
        "!nbstripout --install\n",
        "!git branch -a\n",
        "\n",
        "\n",
        "# Install dependencies if present\n",
        "if os.path.exists(\"requirements.txt\"):\n",
        "    %pip install -r requirements.txt\n",
        "if os.path.exists(\"pyproject.toml\"):\n",
        "    %pip install -e .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8kbgaQQ6fFfM",
      "metadata": {
        "collapsed": true,
        "id": "8kbgaQQ6fFfM"
      },
      "outputs": [],
      "source": [
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "WANDB_TOKEN = userdata.get('WANDB_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_TOKEN\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "wandb.login(key=WANDB_TOKEN, relogin=True)\n",
        "login(token=HF_TOKEN, add_to_git_credential=True)  # also sets Git creds for LFS\n",
        "\n",
        "wandb.init(\n",
        "    project=\"qwen3coder-sft-5ep\",\n",
        "    name=f\"sfttrainer-5ep-run-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "    settings=wandb.Settings(ignore_globs=[\"*.bin\",\"*.pt\",\"*.safetensors\",\"*.ckpt\",\"checkpoint*\"])\n",
        ")\n",
        "wandb.define_metric(\"train/global_step\")\n",
        "wandb.define_metric(\"train/*\", step_metric=\"train/global_step\")\n",
        "wandb.define_metric(\"eval/*\",  step_metric=\"train/global_step\")\n",
        "\n",
        "print(\"Logged in as:\", whoami(token=HF_TOKEN)[\"name\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4349459",
      "metadata": {
        "id": "c4349459"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, json, random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Disable Torch Dynamo / torch.compile to avoid environment-specific issues.\n",
        "os.environ.setdefault(\"TORCH_COMPILE_DISABLE\", \"1\")\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    Trainer, TrainingArguments, BitsAndBytesConfig\n",
        ")\n",
        "\n",
        "print(\"Python:\", os.sys.version)\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Device:\", torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8c540f4e",
      "metadata": {
        "id": "8c540f4e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ==== USER CONFIG ====\n",
        "MODEL_ID = \"Qwen/Qwen3-Coder-30B-A3B-Instruct\"\n",
        "ADAPTER_ID = \"tam2003/Qwen3-Coder-30b-v5-2ep\"\n",
        "TRUST_REMOTE_CODE = True  # set True for models (e.g., Qwen) that need remote code\n",
        "OUTPUT_DIR = \"tam2003/SFT-Qwen3-30B-dsetV3\"\n",
        "\n",
        "# Data: provide a path to a JSONL/JSON with a `messages` field per record\n",
        "JSONL_PATH = \"/content/Cryptol_SFT_message_format_nocomments_source_code_V3.jsonl\"   # e.g., \"/content/my_data.jsonl\" (leave blank to use toy dataset)\n",
        "\n",
        "# Training hyperparameters\n",
        "MAX_SEQ_LEN = 4096            # adjust to your context length + memory\n",
        "BATCH_SIZE = 2                # per-device train batch size\n",
        "GRAD_ACC = 4\n",
        "EPOCHS = 5\n",
        "LR = 2e-4\n",
        "EVAL_STEPS = 30\n",
        "SAVE_STEPS = 30\n",
        "LOG_STEPS = 30\n",
        "\n",
        "# LoRA options\n",
        "USE_LORA = True\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05\n",
        "# Narrow target modules list if memory-limited. Common: q_proj, v_proj, k_proj, o_proj\n",
        "#LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "LORA_TARGET_MODULES = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        "#LORA_TARGET_MODULES = [\"q_proj\",\"k_proj\",\"v_proj\"]  # lora_target_modules\n",
        "\n",
        "# bitsandbytes config\n",
        "USE_NESTED_QUANT = True  # use_nested_quant\n",
        "BNB_4BIT_COMPUTE_DTYPE = \"bfloat16\"  # bnb_4bit_compute_dtype\n",
        "\n",
        "# 4-bit options\n",
        "USE_4BIT = True\n",
        "BNB_COMPUTE_DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "torch.set_float32_matmul_precision(\"high\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00b5aca6",
      "metadata": {
        "id": "00b5aca6"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, trust_remote_code=TRUST_REMOTE_CODE)\n",
        "# Ensure pad token exists\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "print(\"Pad token id:\", tokenizer.pad_token_id, \"EOS:\", tokenizer.eos_token_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "188ef2cb",
      "metadata": {
        "id": "188ef2cb"
      },
      "outputs": [],
      "source": [
        "from src.data import load_or_make_dataset, _chunk_messages_by_tokens, _normalize_messages, explode_long_conversations\n",
        "\n",
        "raw = load_or_make_dataset(JSONL_PATH)\n",
        "raw = raw.filter(lambda ex: ex[\"set\"] in [\"unsupervised\", \"supervised\"])\n",
        "print(raw[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11505ffa",
      "metadata": {
        "collapsed": true,
        "id": "11505ffa"
      },
      "outputs": [],
      "source": [
        "# --- actually apply the splitting to `raw` using MAX_SEQ_LEN = 4096 ---\n",
        "raw = explode_long_conversations(raw, tokenizer, MAX_SEQ_LEN)\n",
        "print(\"After splitting:\")\n",
        "print(raw[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78bb69b9",
      "metadata": {
        "id": "78bb69b9"
      },
      "outputs": [],
      "source": [
        "def encode_chat_last_assistant_only(messages: List[Dict[str, Any]], max_len: int):\n",
        "    msgs = _normalize_messages(messages)\n",
        "\n",
        "    # find the last assistant turn\n",
        "    last_asst_idx = -1\n",
        "    for i in range(len(msgs) - 1, -1, -1):\n",
        "        if msgs[i].get(\"role\") == \"assistant\":\n",
        "            last_asst_idx = i\n",
        "            break\n",
        "\n",
        "    if last_asst_idx == -1:\n",
        "        # no assistant -> skip this example\n",
        "        return None\n",
        "\n",
        "    # history without the last assistant, and history with it\n",
        "    hist_wo = msgs[:last_asst_idx]          # may be []\n",
        "    hist_w  = msgs[:last_asst_idx + 1]      # at least one message (the assistant)\n",
        "\n",
        "    # handle empty history safely\n",
        "    if hist_wo:\n",
        "        ids_hist = tokenizer.apply_chat_template(\n",
        "            hist_wo,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=False,\n",
        "        )\n",
        "    else:\n",
        "        ids_hist = []\n",
        "\n",
        "    # full sequence including last assistant\n",
        "    ids_full = tokenizer.apply_chat_template(\n",
        "        hist_w,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=False,\n",
        "    )\n",
        "\n",
        "    # Ensure EOS at the end (optional but nice)\n",
        "    if tokenizer.eos_token_id is not None and (len(ids_full) == 0 or ids_full[-1] != tokenizer.eos_token_id):\n",
        "        ids_full = ids_full + [tokenizer.eos_token_id]\n",
        "\n",
        "    input_ids = ids_full\n",
        "\n",
        "    # labels: mask out the history, keep only the last assistant tokens\n",
        "    labels = [-100] * len(ids_hist) + input_ids[len(ids_hist):]\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Truncate from the left if needed\n",
        "    if len(input_ids) > max_len:\n",
        "        input_ids = input_ids[-max_len:]\n",
        "        attention_mask = attention_mask[-max_len:]\n",
        "        labels = labels[-max_len:]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "\n",
        "def ds_map_fn(ex):\n",
        "    out = encode_chat_last_assistant_only(ex[\"messages\"], MAX_SEQ_LEN)\n",
        "    if out is None:\n",
        "        # IMPORTANT: always return all keys, even for rows we will drop\n",
        "        return {\n",
        "            \"input_ids\": [],\n",
        "            \"attention_mask\": [],\n",
        "            \"labels\": [],\n",
        "            \"_drop\": True,\n",
        "        }\n",
        "    out[\"_drop\"] = False\n",
        "    return out\n",
        "\n",
        "\n",
        "raw = raw.class_encode_column(\"filetype\")\n",
        "\n",
        "split = raw.train_test_split(test_size=0.2, seed=42, stratify_by_column=\"filetype\")\n",
        "train_raw, eval_raw = split[\"train\"], split[\"test\"]\n",
        "\n",
        "train_ds = train_raw.map(\n",
        "    ds_map_fn,\n",
        "    remove_columns=[c for c in train_raw.column_names if c != \"messages\"],\n",
        ")\n",
        "train_ds = train_ds.filter(lambda ex: ex[\"_drop\"] is False).remove_columns(\"_drop\")\n",
        "\n",
        "eval_ds = eval_raw.map(\n",
        "    ds_map_fn,\n",
        "    remove_columns=[c for c in eval_raw.column_names if c != \"messages\"],\n",
        ")\n",
        "eval_ds = eval_ds.filter(lambda ex: ex[\"_drop\"] is False).remove_columns(\"_drop\")\n",
        "\n",
        "# quick inspection\n",
        "print(\"Train example lens:\", len(train_ds[0][\"input_ids\"]), len(train_ds[0][\"labels\"]))\n",
        "print(\"Eval example lens:\", len(eval_ds[0][\"input_ids\"]), len(eval_ds[0][\"labels\"]))\n",
        "\n",
        "max_train_len = max(len(ex[\"input_ids\"]) for ex in train_ds)\n",
        "max_eval_len  = max(len(ex[\"input_ids\"]) for ex in eval_ds)\n",
        "print(\"Max train len:\", max_train_len)\n",
        "print(\"Max eval  len:\", max_eval_len)\n",
        "\n",
        "print(train_ds, eval_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7d76455c",
      "metadata": {
        "id": "7d76455c"
      },
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class DataCollatorForCausalLMWithLabels:\n",
        "    tokenizer: AutoTokenizer\n",
        "    label_pad_token_id: int = -100\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]):\n",
        "        # Separate out labels for manual padding\n",
        "        labels = [f[\"labels\"] for f in features]\n",
        "        batch_inputs = {\n",
        "            \"input_ids\": [f[\"input_ids\"] for f in features],\n",
        "            \"attention_mask\": [f[\"attention_mask\"] for f in features]\n",
        "        }\n",
        "        padded = self.tokenizer.pad(\n",
        "            batch_inputs, padding=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        max_len = padded[\"input_ids\"].shape[1]\n",
        "        padded_labels = torch.full(\n",
        "            (len(labels), max_len), self.label_pad_token_id, dtype=torch.long\n",
        "        )\n",
        "        for i, lab in enumerate(labels):\n",
        "            padded_labels[i, :len(lab)] = torch.tensor(lab, dtype=torch.long)\n",
        "        padded[\"labels\"] = padded_labels\n",
        "        return padded\n",
        "\n",
        "data_collator = DataCollatorForCausalLMWithLabels(tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "AOs5EC2y2e8U",
      "metadata": {
        "id": "AOs5EC2y2e8U"
      },
      "outputs": [],
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "gc.collect(); torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d423ff02",
      "metadata": {
        "id": "d423ff02"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 4-bit quantization\n",
        "compute_dtype = getattr(torch, BNB_4BIT_COMPUTE_DTYPE)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=USE_NESTED_QUANT,\n",
        ")\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        load_in_8bit=False,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        use_cache=False,\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"flash_attention_2\",\n",
        ")\n",
        "\n",
        "base = prepare_model_for_kbit_training(base)\n",
        "\n",
        "# Ensure model has pad token id in config for generation convenience\n",
        "if getattr(base.config, \"pad_token_id\", None) is None:\n",
        "    base.config.pad_token_id = tokenizer.pad_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yYITGIQw1pvi",
      "metadata": {
        "id": "yYITGIQw1pvi"
      },
      "outputs": [],
      "source": [
        "blk = base.model.layers[0]           # Llama/Qwen-style\n",
        "print(\"ATTN:\", blk.self_attn)         # has q_proj, k_proj, v_proj, o_proj\n",
        "print(\"MLP:\", blk.mlp)\n",
        "target_modules = LORA_TARGET_MODULES\n",
        "# LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "# LORA_TARGET_MODULES = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51d38b8f",
      "metadata": {
        "id": "51d38b8f"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=LORA_TARGET_MODULES,\n",
        "    bias=\"none\",\n",
        ")\n",
        "# model = PeftModel.from_pretrained(base, \"HUGGING FACE ADAPTERS\")\n",
        "model = PeftModel.from_pretrained(base, ADAPTER_ID, is_trainable=True)\n",
        "\n",
        "# model = get_peft_model(base, lora_cfg)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "407562e4",
      "metadata": {
        "id": "407562e4"
      },
      "outputs": [],
      "source": [
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACC,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LR,\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=LOG_STEPS,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    save_total_limit=2,\n",
        "    bf16=torch.cuda.is_available(),\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=[\"wandb\"],\n",
        "    load_best_model_at_end=True,\n",
        "    hub_model_id=OUTPUT_DIR,\n",
        "    hub_strategy=\"every_save\",\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[\n",
        "        EarlyStoppingCallback(\n",
        "            early_stopping_patience=3,\n",
        "            early_stopping_threshold=0.0\n",
        "        )\n",
        "    ],\n",
        ")\n",
        "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": True})\n",
        "model.config.use_cache = False\n",
        "model.config.attn_implementation = \"flash_attention_2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GLtE5H18ksYW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "GLtE5H18ksYW",
        "outputId": "193004dd-4204-4f6b-8231-e4716485d150"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Casting fp32 inputs back to torch.bfloat16 for flash-attn compatibility.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='66' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 66/300 3:44:22 < 13:40:23, 0.00 it/s, Epoch 1.08/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.813700</td>\n",
              "      <td>0.716610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.609400</td>\n",
              "      <td>0.663504</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(\"Training...\")\n",
        "try:\n",
        "  trainer.train(resume_from_checkpoint=False)\n",
        "  eval_results = trainer.evaluate()\n",
        "except Exception as e:\n",
        "  print(f\"Training failed: {e}\")\n",
        "finally:\n",
        "  trainer.save_state()\n",
        "  trainer.save_model(OUTPUT_DIR)\n",
        "  print(\"Saved model to:\", OUTPUT_DIR)\n",
        "\n",
        "wandb.finish()\n",
        "eval_loss = eval_results[\"eval_loss\"]\n",
        "perplexity = math.exp(eval_loss)\n",
        "print(f\"Eval loss = {eval_loss:.2f}, Perplexity = {perplexity:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "OCCubwXt1GbE"
      },
      "id": "OCCubwXt1GbE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4235b2f3",
      "metadata": {
        "id": "4235b2f3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Quick smoke test generation (greedy/short)\n",
        "model.eval()\n",
        "prompt_msgs = [\n",
        "    {\"role\": \"system\", \"content\": \"Return exactly ONE fenced code block labeled `cryptol` and nothing else (no prose before/after).\"},\n",
        "    {\"role\": \"user\", \"content\": \"Implement a Caesar cipher. Define the functions `encrypt` and `decrypt` with the signature: `{n} [8] -> [n][8] -> [n][8]`.\"}\n",
        "]\n",
        "\n",
        "# add_generation_prompt=True -> model should produce assistant continuation\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    prompt_msgs, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=500,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "generated = out[0, inputs.shape[-1]:]\n",
        "print(tokenizer.decode(generated, skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HPtBr11E-zD3",
      "metadata": {
        "id": "HPtBr11E-zD3"
      },
      "outputs": [],
      "source": [
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df4aabd1",
      "metadata": {
        "id": "df4aabd1"
      },
      "source": [
        "\n",
        "## Notes & References\n",
        "- ðŸ¤— Transformers `Trainer` docs (training loop, arguments)  \n",
        "  https://huggingface.co/docs/transformers/main_classes/trainer\n",
        "- `apply_chat_template` for model-specific chat formatting  \n",
        "  https://huggingface.co/docs/transformers/main/chat_templating\n",
        "- PEFT / LoRA docs  \n",
        "  https://huggingface.co/docs/peft\n",
        "- BitsAndBytes 4-bit quantization  \n",
        "  https://github.com/TimDettmers/bitsandbytes\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}