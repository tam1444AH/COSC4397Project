{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPj9Rz6bOZa4sagEx0q5iYF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/COSC4397Project/blob/main/notebooks/supervised-data-preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U \"huggingface-hub<1.0\" accelerate peft bitsandbytes\n",
        "!pip -q install -U ms-swift transformers"
      ],
      "metadata": {
        "id": "-3Bk5TE_p1nN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cffabe8-2223-4064-8ebe-6b959402601c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.8/298.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.6/449.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m882.8/882.8 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m133.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.6/564.6 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.5/99.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for binpacking (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers-stream-generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, transformers, huggingface_hub, sys, os\n",
        "os.environ[\"PATH\"] += \":/root/.local/bin:/usr/local/bin\"\n",
        "print(\"swift bin:\", shutil.which(\"swift\"))\n",
        "print(\"ms-swift bin:\", shutil.which(\"ms-swift\"))\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"huggingface_hub:\", huggingface_hub.__version__)"
      ],
      "metadata": {
        "id": "a8mov5Caq0Qq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "694a8064-bac0-485a-b6b9-7988d6695758"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "swift bin: /usr/local/bin/swift\n",
            "ms-swift bin: None\n",
            "transformers: 4.57.1\n",
            "huggingface_hub: 0.36.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U hf_transfer\n",
        "!export HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login, whoami\n",
        "import wandb\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # mitigate fragmentation\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "WANDB_TOKEN = userdata.get('WANDB_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_TOKEN\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "wandb.login(key=WANDB_TOKEN, relogin=True)\n",
        "login(token=HF_TOKEN, add_to_git_credential=True)  # also sets Git creds for LFS\n",
        "\n",
        "print(\"Logged in as:\", whoami(token=HF_TOKEN)[\"name\"])"
      ],
      "metadata": {
        "id": "Cp0_l1uxh36L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e89144-ecab-455a-dead-66defe6ee2d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammedtalha290\u001b[0m (\u001b[33mmohammedtalha290-university-of-houston\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged in as: tam2003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, random\n",
        "from collections import defaultdict\n",
        "random.seed(42)\n",
        "\n",
        "dataset_path = Path(\"/content/test.jsonl\")\n",
        "rows = [json.loads(line) for line in dataset_path.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
        "\n",
        "supervised_rows = [row for row in rows if row.get(\"set\") == \"supervised\"]\n",
        "print(f\"Total rows: {len(rows)}, Supervised rows: {len(supervised_rows)}\")\n",
        "\n",
        "if len(supervised_rows) == 0:\n",
        "    print(\"WARNING: No supervised examples found! Check your 'set' field.\")\n",
        "    supervised_rows = rows\n",
        "\n",
        "by_filetype = defaultdict(list)\n",
        "for row in rows:\n",
        "    filetype = row.get(\"filetype\", \"unknown\")\n",
        "    by_filetype[filetype].append(row)\n",
        "\n",
        "train, val = [], []\n",
        "for filetype, examples in by_filetype.items():\n",
        "    random.shuffle(examples)\n",
        "    cut = max(1, int(0.95 * len(examples)))\n",
        "    train.extend(examples[:cut])\n",
        "    val.extend(examples[cut:])\n",
        "    print(f\"{filetype}: {len(examples)} total → {len(examples[:cut])} train, {len(examples[cut:])} val\")\n",
        "\n",
        "random.shuffle(train)\n",
        "random.shuffle(val)\n",
        "\n",
        "train_path = Path(\"/content/train.jsonl\")\n",
        "val_path = Path(\"/content/val.jsonl\")\n",
        "train_path.write_text(\"\\n\".join(json.dumps(row, ensure_ascii=False) for row in train), encoding=\"utf-8\")\n",
        "val_path.write_text(\"\\n\".join(json.dumps(row, ensure_ascii=False) for row in val), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"\\nFinal split - Train: {len(train)}, Val: {len(val)}\")\n",
        "print(f\"Saved to {train_path} and {val_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O-kVcwGMsca",
        "outputId": "8ce1fbba-0326-4428-c6a7-2a9623b416d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 75, Supervised rows: 18\n",
            "cryptol: 75 total → 71 train, 4 val\n",
            "\n",
            "Final split - Train: 71, Val: 4\n",
            "Saved to /content/train.jsonl and /content/val.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "export SWIFT_USE_HF=1\n",
        "\n",
        "swift sft \\\n",
        "  --use_hf true \\\n",
        "  --model Qwen/Qwen2.5-Coder-32B \\\n",
        "  --adapters tam2003/peft-FT-2.5-Coder-32b \\\n",
        "  --dataset /content/train.jsonl \\\n",
        "  --val_dataset /content/val.jsonl \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --warmup_ratio 0.03 \\\n",
        "  --per_device_train_batch_size 2 \\\n",
        "  --gradient_accumulation_steps 8 \\\n",
        "  --torch_dtype float16 \\\n",
        "  --gradient_checkpointing true \\\n",
        "  --lora_r 16 \\\n",
        "  --lora_alpha 32 \\\n",
        "  --lora_dropout 0.05 \\\n",
        "  --save_steps 200 \\\n",
        "  --logging_steps 20 \\\n",
        "  --output_dir /content/sft_output"
      ],
      "metadata": {
        "id": "RiSemxr2jWbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U trl datasets\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, TaskType"
      ],
      "metadata": {
        "id": "Dj8RuRHmZjI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen3-32B-Coder-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "train_dataset = load_dataset(\"json\", data_files=\"/content/train.jsonl\", split=\"train\")\n",
        "val_dataset = load_dataset(\"json\", data_files=\"/content/val.jsonl\", split=\"train\")\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/sft_output\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=20,\n",
        "    save_steps=200,\n",
        "    eval_steps=200,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"wandb\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"your-username/qwen3-coder-cryptol-sft\",\n",
        "    hub_token=HF_TOKEN,\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save and push\n",
        "trainer.save_model(\"/content/sft_output/final\")\n",
        "trainer.push_to_hub()\n"
      ],
      "metadata": {
        "id": "Ifb209uuZuVp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}