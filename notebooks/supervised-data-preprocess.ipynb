{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMWaf7uoqPYxrth5Hq5L57G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/COSC4397Project/blob/main/notebooks/supervised-data-preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nbstripout\n",
        "%pip install -U \"huggingface-hub>=0.34.0,<1.0\"\n",
        "%pip check\n",
        "%pip install hf_transfer\n",
        "%pip install -U bitsandbytes --upgrade\n",
        "%pip install transformers datasets\n",
        "%pip install transformers datasets peft flash-attn trl\n",
        "!export HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import json, random\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "import os, math, torch\n",
        "import wandb\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login, whoami\n",
        "from time import time\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "from peft import PeftModel\n",
        "\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"false\"  # or \"true\" to mute\n",
        "os.environ[\"WANDB_PROJECT\"]   = \"qwen3coder-finetune-fp16\"\n",
        "\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # mitigate fragmentation\n",
        "REPO_URL=\"https://github.com/UH-Insure/Finetuning-Qwen3.git\"\n",
        "REPO=\"Finetuning-Qwen3\"\n",
        "\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "# If repo exists, update it; otherwise, clone fresh\n",
        "if os.path.exists(REPO):\n",
        "    print(f\"Repo '{REPO}' exists, pulling latest changes...\")\n",
        "    os.chdir(REPO)\n",
        "    !git reset --hard HEAD   # optional: discard local changes\n",
        "    !git pull\n",
        "else:\n",
        "    print(f\"Cloning repo '{REPO}'...\")\n",
        "    !git clone \"$REPO_URL\" \"$REPO\"\n",
        "    os.chdir(REPO)\n",
        "\n",
        "!nbstripout --install\n",
        "!git branch -a\n",
        "\n",
        "\n",
        "# Install dependencies if present\n",
        "if os.path.exists(\"requirements.txt\"):\n",
        "    %pip install -r requirements.txt\n",
        "if os.path.exists(\"pyproject.toml\"):\n",
        "    %pip install -e ."
      ],
      "metadata": {
        "id": "gDuvjpDDzw8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 4371\n",
        "data = \"/content/\"\n",
        "base = \"Qwen/Qwen3-Coder-30B-Instruct\"\n",
        "adapter_name = \"tam2003/Qwen3-Coder-30b-v5-2ep\"\n",
        "output_dir = \"tam2003/Qwen3-Coder-30b-v5-2ep-sft\"\n",
        "epochs = 1\n",
        "per_dev_bs = 5\n",
        "grad_acc = 2\n",
        "lr = 5e-5\n",
        "warmup_ratio = 0.03\n",
        "max_seq_len = 4096"
      ],
      "metadata": {
        "id": "Y6rWiifrSuBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_PROJECT\"] = \"qwen3-sft-test\"\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "WANDB_TOKEN = userdata.get('WANDB_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_TOKEN\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "wandb.login(key=WANDB_TOKEN, relogin=True)\n",
        "login(token=HF_TOKEN, add_to_git_credential=True)  # also sets Git creds for LFS\n",
        "\n",
        "print(\"Logged in as:\", whoami(token=HF_TOKEN)[\"name\"])\n",
        "wandb.init(\n",
        "    project=os.environ[\"WANDB_PROJECT\"],\n",
        "    name=f\"sfttrainer-1ep-resume-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "    settings=wandb.Settings(ignore_globs=[\"*.bin\",\"*.pt\",\"*.safetensors\",\"*.ckpt\",\"checkpoint*\"])\n",
        ")\n",
        "wandb.define_metric(\"train/global_step\")\n",
        "wandb.define_metric(\"train/*\", step_metric=\"train/global_step\")\n",
        "wandb.define_metric(\"eval/*\",  step_metric=\"train/global_step\")\n",
        "\n",
        "random.seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")"
      ],
      "metadata": {
        "id": "4drVOVn7WJ_Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}