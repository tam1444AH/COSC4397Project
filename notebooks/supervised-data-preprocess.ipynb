{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOmsmL1fQTzO4vU0vgA47g7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/COSC4397Project/blob/main/notebooks/supervised-data-preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U hf_transfer\n",
        "!export HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login, whoami\n",
        "import wandb\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # mitigate fragmentation\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "WANDB_TOKEN = userdata.get('WANDB_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_TOKEN\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "wandb.login(key=WANDB_TOKEN, relogin=True)\n",
        "login(token=HF_TOKEN, add_to_git_credential=True)  # also sets Git creds for LFS\n",
        "\n",
        "print(\"Logged in as:\", whoami(token=HF_TOKEN)[\"name\"])"
      ],
      "metadata": {
        "id": "TKxjbGr2dW8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U trl datasets\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, TaskType\n",
        "\n",
        "wandb.init(\n",
        "    project=\"qwen3coder-finetune-fp16-talha-v3-sft\",\n",
        "    name=f\"run-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "    settings=wandb.Settings(\n",
        "      ignore_globs=[\"*.bin\",\"*.pt\",\"*.safetensors\",\"*.ckpt\",\"checkpoint*\"]\n",
        "))\n",
        "\n",
        "wandb.define_metric(\"train/global_step\")\n",
        "wandb.define_metric(\"train/*\", step_metric=\"train/global_step\")\n",
        "wandb.define_metric(\"eval/*\",  step_metric=\"train/global_step\")"
      ],
      "metadata": {
        "id": "p4hJgW9sdd7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, random\n",
        "from collections import defaultdict\n",
        "random.seed(4371)\n",
        "\n",
        "dataset_path = Path(\"/content/test.jsonl\") # This will be our raw dataset.\n",
        "rows = [json.loads(line) for line in dataset_path.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
        "\n",
        "supervised_rows = [row for row in rows if row.get(\"set\") == \"supervised\"]\n",
        "print(f\"Total rows: {len(rows)}, Supervised rows: {len(supervised_rows)}\")\n",
        "\n",
        "by_filetype = defaultdict(list)\n",
        "\n",
        "for row in supervised_rows:\n",
        "  filetype = row.get(\"filetype\")\n",
        "  by_filetype[filetype].append(row)\n",
        "\n",
        "train, val = [], []\n",
        "\n",
        "for filetype, supervised_rows in by_filetype.items():\n",
        "  random.shuffle(supervised_rows)\n",
        "  cut = max(1, int(0.95 * len(supervised_rows)))\n",
        "  train.extend(supervised_rows[:cut])\n",
        "  val.extend(supervised_rows[cut:])\n",
        "\n",
        "random.shuffle(train)\n",
        "random.shuffle(val)\n",
        "\n",
        "train_path = Path(\"/content/train.jsonl\")\n",
        "val_path = Path(\"/content/val.jsonl\")\n",
        "\n",
        "train_path.write_text(\"\\n\".join(json.dumps(row, ensure_ascii=False) for row in train), encoding=\"utf-8\")\n",
        "val_path.write_text(\"\\n\".join(json.dumps(row, ensure_ascii=False) for row in val), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"\\nFinal split - Train: {len(train)}, Val: {len(val)}\")\n",
        "print(f\"Saved to {train_path} and {val_path}\")\n"
      ],
      "metadata": {
        "id": "a_mgP326bxLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U bitsandbytes\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from peft.tuners.lora import LoraLayer\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "model_name = \"Qwen/Qwen3-32B-Coder-Instruct\"\n",
        "adapter_name = \"tam2003/Qwen3-Coder-30b-v5-2ep\"\n",
        "output_dir = \"Qwen3-Coder-30b-v5-2ep\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    padding_side=\"right\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        "    trust_remote_code=True,\n",
        "    use_fast=False,\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "names = [\"<|fim_prefix|>\", \"<|fim_middle|>\", \"<|fim_suffix|>\", \"<|fim_pad|>\"]\n",
        "ids = [tokenizer.convert_tokens_to_ids(t) for t in names]\n",
        "print(dict(zip(names, ids)))\n",
        "print(\"additional_special_tokens:\", tokenizer.special_tokens_map.get(\"additional_special_tokens\"))\n",
        "\n",
        "BNB_4BIT_COMPUTE_DTYPE = \"bfloat16\"\n",
        "\n",
        "compute_dtype = getattr(torch, BNB_4BIT_COMPUTE_DTYPE)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_quant_type=\"nf4\",\n",
        "  bnb_4bit_compute_dtype=compute_dtype,\n",
        "  bnb_4bit_use_double_quant=USE_NESTED_QUANT,\n",
        ")\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "  model_name,\n",
        "  load_in_8bit=False,\n",
        "  quantization_config=bnb_config,\n",
        "  dtype=torch.bfloat16,\n",
        "  device_map=\"auto\",\n",
        "  use_cache=False,\n",
        "  trust_remote_code=True,\n",
        "  attn_implementation=\"flash_attention_2\",\n",
        ")\n",
        "\n",
        "train_dataset = load_dataset(\"json\", data_files=\"/content/train.jsonl\", split=\"train\")\n",
        "val_dataset = load_dataset(\"json\", data_files=\"/content/val.jsonl\", split=\"train\")"
      ],
      "metadata": {
        "id": "F5cwAwIWdlBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")"
      ],
      "metadata": {
        "id": "XQVV-5qUdljf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(base, adapter_name, is_trainable=True)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "ITrw5fLejsM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=5,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_ratio=0.03,\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=[\"wandb\"],\n",
        "    push_to_hub=False,\n",
        ")"
      ],
      "metadata": {
        "id": "pyvyfOJvdq_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=4096,\n",
        ")"
      ],
      "metadata": {
        "id": "ASscc2H3TfEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_start_time = time()\n",
        "print(\"Training...\")\n",
        "\n",
        "try:\n",
        "  trainer.train(resume_from_checkpoint=False)\n",
        "  train_end_time = time()\n",
        "  print(f\"Training completed in {train_end_time - train_start_time:.2f} seconds.\")\n",
        "  eval_results = trainer.evaluate()\n",
        "except e as Exception:\n",
        "  print(f\"Training failed: {e}\")\n",
        "finally:\n",
        "  trainer.save_state()\n",
        "  trainer.save_model(f\"{trainer.args.output_dir}/last-safe\")\n",
        "wandb.finish()\n",
        "\n",
        "\n",
        "if \"eval_loss\" in eval_results and math.isfinite(eval_results[\"eval_loss\"]):\n",
        "  eval_loss = eval_results[\"eval_loss\"]\n",
        "  ppl = math.exp(eval_loss)\n",
        "  print(f\"Eval loss = {eval_loss:.4f}, Perplexity = {ppl:.4f}\")\n"
      ],
      "metadata": {
        "id": "LHiGGOjPdtW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "Dn5DSUj-duUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "yor6_ZyB8K6I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}