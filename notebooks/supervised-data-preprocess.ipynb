{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPtFfZQlzyf5jdBGw0J1tgg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/COSC4397Project/blob/main/notebooks/supervised-data-preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nbstripout\n",
        "%pip install -U \"huggingface-hub>=0.34.0,<1.0\"\n",
        "%pip check\n",
        "%pip install hf_transfer\n",
        "%pip install -U bitsandbytes --upgrade\n",
        "%pip install transformers datasets\n",
        "%pip install transformers datasets peft flash-attn\n",
        "!export HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import json, random\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "import wandb\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login, whoami\n",
        "os.environ[\"WANDB_DISABLED\"] = \"false\"  # or \"true\" to mute\n",
        "os.environ[\"WANDB_PROJECT\"]   = \"qwen3coder-finetune-fp16\"\n",
        "\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # mitigate fragmentation\n",
        "REPO_URL=\"https://github.com/UH-Insure/Finetuning-Qwen3.git\"\n",
        "REPO=\"Finetuning-Qwen3\"\n",
        "\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "# If repo exists, update it; otherwise, clone fresh\n",
        "if os.path.exists(REPO):\n",
        "    print(f\"Repo '{REPO}' exists, pulling latest changes...\")\n",
        "    os.chdir(REPO)\n",
        "    !git reset --hard HEAD   # optional: discard local changes\n",
        "    !git pull\n",
        "else:\n",
        "    print(f\"Cloning repo '{REPO}'...\")\n",
        "    !git clone \"$REPO_URL\" \"$REPO\"\n",
        "    os.chdir(REPO)\n",
        "\n",
        "!nbstripout --install\n",
        "!git branch -a\n",
        "\n",
        "\n",
        "# Install dependencies if present\n",
        "if os.path.exists(\"requirements.txt\"):\n",
        "    %pip install -r requirements.txt\n",
        "if os.path.exists(\"pyproject.toml\"):\n",
        "    %pip install -e ."
      ],
      "metadata": {
        "id": "gDuvjpDDzw8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U \"transformers>=4.43\" \"datasets>=2.20\" peft bitsandbytes trl wandb\n",
        "\n",
        "import os, math, json, random, torch\n",
        "from time import time\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "Pplosv_0zyue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}