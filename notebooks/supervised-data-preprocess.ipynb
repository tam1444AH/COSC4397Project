{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOp2kdHB8OSzefRf9cTuQv1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/COSC4397Project/blob/main/notebooks/supervised-data-preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U hf_transfer\n",
        "!export HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login, whoami\n",
        "import wandb\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # mitigate fragmentation\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "WANDB_TOKEN = userdata.get('WANDB_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_TOKEN\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "wandb.login(key=WANDB_TOKEN, relogin=True)\n",
        "login(token=HF_TOKEN, add_to_git_credential=True)  # also sets Git creds for LFS\n",
        "\n",
        "print(\"Logged in as:\", whoami(token=HF_TOKEN)[\"name\"])"
      ],
      "metadata": {
        "id": "TKxjbGr2dW8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a4ebff-b97d-4ba8-fec1-8f05fc0b2925"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammedtalha290\u001b[0m (\u001b[33mmohammedtalha290-university-of-houston\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logged in as: tam2003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U trl datasets\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, TaskType\n",
        "\n",
        "wandb.init(\n",
        "    project=\"qwen3coder-finetune-fp16-talha-v3-sft\",\n",
        "    name=f\"run-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "    settings=wandb.Settings(\n",
        "      ignore_globs=[\"*.bin\",\"*.pt\",\"*.safetensors\",\"*.ckpt\",\"checkpoint*\"]\n",
        "))\n",
        "\n",
        "wandb.define_metric(\"train/global_step\")\n",
        "wandb.define_metric(\"train/*\", step_metric=\"train/global_step\")\n",
        "wandb.define_metric(\"eval/*\",  step_metric=\"train/global_step\")"
      ],
      "metadata": {
        "id": "p4hJgW9sdd7h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "4f535d01-bc2b-4c4c-fea9-9be1a5d4de07"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/462.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'datetime' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2525231655.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m wandb.init(\n\u001b[1;32m     10\u001b[0m     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"qwen3coder-finetune-fp16-talha-v3-sft\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"run-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     settings=wandb.Settings(\n\u001b[1;32m     13\u001b[0m       \u001b[0mignore_globs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"*.bin\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"*.pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"*.safetensors\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"*.ckpt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"checkpoint*\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, random\n",
        "from collections import defaultdict\n",
        "random.seed(4371)\n",
        "\n",
        "dataset_path = Path(\"/content/SFT_message_format_hybrid_source_code_V2.jsonl\") # This will be our raw dataset.\n",
        "rows = [json.loads(line) for line in dataset_path.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
        "\n",
        "supervised_rows = [row for row in rows if row.get(\"set\") == \"supervised\"]\n",
        "print(f\"Total rows: {len(rows)}, Supervised rows: {len(supervised_rows)}\")\n",
        "\n",
        "by_filetype = defaultdict(list)\n",
        "\n",
        "for row in supervised_rows:\n",
        "  filetype = row.get(\"filetype\")\n",
        "  by_filetype[filetype].append(row)\n",
        "\n",
        "train, val = [], []\n",
        "\n",
        "for filetype, supervised_rows in by_filetype.items():\n",
        "  random.shuffle(supervised_rows)\n",
        "  cut = max(1, int(0.95 * len(supervised_rows)))\n",
        "  train.extend(supervised_rows[:cut])\n",
        "  val.extend(supervised_rows[cut:])\n",
        "\n",
        "random.shuffle(train)\n",
        "random.shuffle(val)\n",
        "\n",
        "train_path = Path(\"/content/train.jsonl\")\n",
        "val_path = Path(\"/content/val.jsonl\")\n",
        "\n",
        "train_path.write_text(\"\\n\".join(json.dumps(row, ensure_ascii=False) for row in train), encoding=\"utf-8\")\n",
        "val_path.write_text(\"\\n\".join(json.dumps(row, ensure_ascii=False) for row in val), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"\\nFinal split - Train: {len(train)}, Val: {len(val)}\")\n",
        "print(f\"Saved to {train_path} and {val_path}\")\n"
      ],
      "metadata": {
        "id": "a_mgP326bxLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U bitsandbytes\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from peft.tuners.lora import LoraLayer\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "model_name = \"Qwen/Qwen3-32B-Coder-Instruct\"\n",
        "adapter_name = \"tam2003/Qwen3-Coder-30b-v5-2ep\"\n",
        "output_dir = \"Qwen3-Coder-30b-v5-2ep\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    padding_side=\"right\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        "    trust_remote_code=True,\n",
        "    use_fast=False,\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "names = [\"<|fim_prefix|>\", \"<|fim_middle|>\", \"<|fim_suffix|>\", \"<|fim_pad|>\"]\n",
        "ids = [tokenizer.convert_tokens_to_ids(t) for t in names]\n",
        "print(dict(zip(names, ids)))\n",
        "print(\"additional_special_tokens:\", tokenizer.special_tokens_map.get(\"additional_special_tokens\"))\n",
        "\n",
        "BNB_4BIT_COMPUTE_DTYPE = \"bfloat16\"\n",
        "\n",
        "compute_dtype = getattr(torch, BNB_4BIT_COMPUTE_DTYPE)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_quant_type=\"nf4\",\n",
        "  bnb_4bit_compute_dtype=compute_dtype,\n",
        "  bnb_4bit_use_double_quant=USE_NESTED_QUANT,\n",
        ")\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "  model_name,\n",
        "  load_in_8bit=False,\n",
        "  quantization_config=bnb_config,\n",
        "  dtype=torch.bfloat16,\n",
        "  device_map=\"auto\",\n",
        "  use_cache=False,\n",
        "  trust_remote_code=True,\n",
        "  attn_implementation=\"flash_attention_2\",\n",
        ")\n",
        "\n",
        "train_dataset = load_dataset(\"json\", data_files=\"/content/train.jsonl\", split=\"train\")\n",
        "val_dataset = load_dataset(\"json\", data_files=\"/content/val.jsonl\", split=\"train\")"
      ],
      "metadata": {
        "id": "F5cwAwIWdlBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")"
      ],
      "metadata": {
        "id": "XQVV-5qUdljf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(base, adapter_name, is_trainable=True)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "ITrw5fLejsM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=5,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_ratio=0.03,\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=[\"wandb\"],\n",
        "    push_to_hub=False,\n",
        ")"
      ],
      "metadata": {
        "id": "pyvyfOJvdq_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=4096,\n",
        ")"
      ],
      "metadata": {
        "id": "ASscc2H3TfEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_start_time = time()\n",
        "print(\"Training...\")\n",
        "\n",
        "try:\n",
        "  trainer.train(resume_from_checkpoint=False)\n",
        "  train_end_time = time()\n",
        "  print(f\"Training completed in {train_end_time - train_start_time:.2f} seconds.\")\n",
        "  eval_results = trainer.evaluate()\n",
        "except e as Exception:\n",
        "  print(f\"Training failed: {e}\")\n",
        "finally:\n",
        "  trainer.save_state()\n",
        "  trainer.save_model(f\"{trainer.args.output_dir}/last-safe\")\n",
        "wandb.finish()\n",
        "\n",
        "\n",
        "if \"eval_loss\" in eval_results and math.isfinite(eval_results[\"eval_loss\"]):\n",
        "  eval_loss = eval_results[\"eval_loss\"]\n",
        "  ppl = math.exp(eval_loss)\n",
        "  print(f\"Eval loss = {eval_loss:.4f}, Perplexity = {ppl:.4f}\")\n"
      ],
      "metadata": {
        "id": "LHiGGOjPdtW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "Dn5DSUj-duUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "yor6_ZyB8K6I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}