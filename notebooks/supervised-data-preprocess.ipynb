{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPUM4MMN/eEmAiHn8ovW/Qh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tam1444AH/COSC4397Project/blob/main/notebooks/supervised-data-preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U hf_transfer\n",
        "!export HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login, whoami\n",
        "import wandb\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # mitigate fragmentation\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "WANDB_TOKEN = userdata.get('WANDB_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = WANDB_TOKEN\n",
        "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
        "wandb.login(key=WANDB_TOKEN, relogin=True)\n",
        "login(token=HF_TOKEN, add_to_git_credential=True)  # also sets Git creds for LFS\n",
        "\n",
        "print(\"Logged in as:\", whoami(token=HF_TOKEN)[\"name\"])"
      ],
      "metadata": {
        "id": "TKxjbGr2dW8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U trl datasets\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, TaskType"
      ],
      "metadata": {
        "id": "p4hJgW9sdd7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, random\n",
        "from collections import defaultdict\n",
        "random.seed(4371)\n",
        "\n",
        "dataset_path = Path(\"/content/test.jsonl\") # This will be our raw dataset.\n",
        "rows = [json.loads(line) for line in dataset_path.read_text(encoding=\"utf-8\").splitlines() if line.strip()]\n",
        "\n",
        "supervised_rows = [row for row in rows if row.get(\"set\") == \"supervised\"]\n",
        "print(f\"Total rows: {len(rows)}, Supervised rows: {len(supervised_rows)}\")\n",
        "\n",
        "by_filetype = defaultdict(list)\n",
        "\n",
        "for row in supervised_rows:\n",
        "  filetype = row.get(\"filetype\")\n",
        "  by_filetype[filetype].append(row)\n",
        "\n",
        "train, val = [], []\n",
        "\n",
        "for filetype, supervised_rows in by_filetype.items():\n",
        "  random.shuffle(supervised_rows)\n",
        "  cut = max(1, int(0.95 * len(supervised_rows)))\n",
        "  train.extend(supervised_rows[:cut])\n",
        "  val.extend(supervised_rows[cut:])\n",
        "\n",
        "random.shuffle(train)\n",
        "random.shuffle(val)\n",
        "\n",
        "train_path = Path(\"/content/train.jsonl\")\n",
        "val_path = Path(\"/content/val.jsonl\")\n",
        "\n",
        "train_path.write_text(\"\\n\".join(json.dumps(row, ensure_ascii=False) for row in train), encoding=\"utf-8\")\n",
        "val_path.write_text(\"\\n\".join(json.dumps(row, ensure_ascii=False) for row in val), encoding=\"utf-8\")\n",
        "\n",
        "print(f\"\\nFinal split - Train: {len(train)}, Val: {len(val)}\")\n",
        "print(f\"Saved to {train_path} and {val_path}\")\n"
      ],
      "metadata": {
        "id": "a_mgP326bxLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen3-32B-Coder-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "train_dataset = load_dataset(\"json\", data_files=\"/content/train.jsonl\", split=\"train\")\n",
        "val_dataset = load_dataset(\"json\", data_files=\"/content/val.jsonl\", split=\"train\")"
      ],
      "metadata": {
        "id": "F5cwAwIWdlBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")"
      ],
      "metadata": {
        "id": "XQVV-5qUdljf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/sft_output\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=5,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_ratio=0.03,\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"wandb\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"\",\n",
        "    hub_token=HF_TOKEN,\n",
        ")"
      ],
      "metadata": {
        "id": "pyvyfOJvdq_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=4096,\n",
        ")"
      ],
      "metadata": {
        "id": "ASscc2H3TfEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "LHiGGOjPdtW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save and push\n",
        "trainer.save_model(\"/content/sft_output/final\")\n",
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "Dn5DSUj-duUR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}